# Sekha Proxy Configuration Example
# Copy this file to .env and fill in your values

# ============================================
# PROXY SETTINGS
# ============================================
PROXY_HOST=0.0.0.0
PROXY_PORT=8081

# ============================================
# LLM SETTINGS
# ============================================
# Provider: ollama, openai, anthropic, google, cohere
LLM_PROVIDER=ollama

# LLM URL
# Ollama: http://localhost:11434
# OpenAI: https://api.openai.com
# Anthropic: https://api.anthropic.com
LLM_URL=http://localhost:11434

# LLM API Key (only for cloud providers)
LLM_API_KEY=

# Timeout in seconds for LLM requests
LLM_TIMEOUT=120

# ============================================
# CONTROLLER SETTINGS
# ============================================
CONTROLLER_URL=http://localhost:8080
CONTROLLER_API_KEY=sk-sekha-your-key-here
CONTROLLER_TIMEOUT=30

# ============================================
# MEMORY SETTINGS
# ============================================
# Enable automatic context injection
AUTO_INJECT_CONTEXT=true

# Max tokens to use for context
CONTEXT_TOKEN_BUDGET=2000

# Max number of conversations to retrieve
CONTEXT_LIMIT=5

# Default folder for auto-captured conversations
DEFAULT_FOLDER=/auto-captured

# Comma-separated list of preferred labels
PREFERRED_LABELS=

# Comma-separated list of folders to exclude from context
EXCLUDED_FOLDERS=/personal/private,/work/confidential

# ============================================
# PRIVACY SETTINGS
# ============================================
# Globally exclude conversations from AI context
EXCLUDE_FROM_AI_CONTEXT=false
